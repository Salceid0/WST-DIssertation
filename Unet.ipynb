{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558c8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from kymatio.torch import Scattering2D\n",
    "import nibabel as nib\n",
    "from scipy.fftpack import fft, ifft, fft2, ifft2, fftshift, ifftshift\n",
    "import PIL.Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "def fftshift2d(x, ifft=False):\n",
    "    assert (len(x.shape) == 2) and all([(s % 2 == 1) for s in x.shape])\n",
    "    s0 = (x.shape[0] // 2) + (0 if ifft else 1)\n",
    "    s1 = (x.shape[1] // 2) + (0 if ifft else 1)\n",
    "    x = np.concatenate([x[s0:, :], x[:s0, :]], axis=0)\n",
    "    x = np.concatenate([x[:, s1:], x[:, :s1]], axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruta = \"C:/Users/javit/Desktop/MRI datasets/datasets/ixi_train-001.pkl\"\n",
    "# ruta_test = \"C:/Users/javit/Desktop/MRI datasets/datasets/ixi_valid.pkl\"\n",
    "ruta = \"C:/Users/javit/Desktop/N2N/datasets/ixi_train.pkl\"\n",
    "ruta_test = \"C:/Users/javit/Desktop/N2N/datasets/ixi_valid.pkl\"\n",
    "img, spec = load_pkl(ruta)\n",
    "img=img[:,:-1,:-1] #images are now 255,255\n",
    "img = img.astype(np.float32) / 255.0 - 0.5 # normalize and make sure they are in range [-.5,.5]\n",
    "test_img, test_spec = load_pkl(ruta_test)\n",
    "test_img=test_img[:,:-1,:-1]\n",
    "test_img=test_img.astype(np.float32) / 255.0 - 0.5\n",
    "\n",
    "\n",
    "p_at_edge=0.025\n",
    "h = [s // 2 for s in (255,255)] #255\n",
    "r = [np.arange(s, dtype=np.float32) - h for s, h in zip((255,255), h)]\n",
    "r = [x ** 2 for x in r]\n",
    "r = (r[0][:, np.newaxis] + r[1][np.newaxis, :]) ** .5\n",
    "m = (p_at_edge ** (1./h[1])) ** r\n",
    "bern_mask = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf0ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_data(img, spec):\n",
    "    global bern_mask\n",
    "    mask = bern_mask\n",
    "    # print('Bernoulli probability at edge = %.5f' % mask[h[0], 0])\n",
    "    # print('Average Bernoulli probability = %.5f' % np.mean(mask))\n",
    "    keep = (np.random.uniform(0.0, 1.0, size=spec.shape)**2 < mask)\n",
    "    keep = keep & keep[::-1, ::-1]\n",
    "    sval = spec * keep\n",
    "    smsk = keep.astype(np.float32)\n",
    "    spec = fftshift2d(sval / (mask + ~keep), ifft=True) # Add 1.0 to not-kept values to prevent div-by-zero.\n",
    "    img = np.real(np.fft.ifft2(spec)).astype(np.float32)\n",
    "    return img, sval, smsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDenoisingDataset(Dataset):\n",
    "    def __init__(self,clean_images,clean_specs,t=64,corrupt_fn=corrupt_data,augment_fn=None):\n",
    "        super(MRIDenoisingDataset, self).__init__()\n",
    "        self.clean_images = clean_images\n",
    "        self.clean_specs = clean_specs\n",
    "        self.t = t\n",
    "        self.corrupt_fn = corrupt_fn\n",
    "        self.augment_fn = augment_fn\n",
    "    def __len__(self):\n",
    "        return len(self.clean_images)\n",
    "    def __getitem__(self, idx):\n",
    "        img_clean= self.clean_images[idx]\n",
    "        spec_clean = self.clean_specs[idx]\n",
    "        # Data augmentation\n",
    "        if self.augment_fn:\n",
    "            img_clean,spec_clean = self.augment_fn(img_clean,spec_clean,t=self.t)        \n",
    "        # Corrupt data\n",
    "        cimg, cspec, cmask = self.corrupt_fn(img_clean,spec_clean)\n",
    "        img_noisy = torch.tensor(cimg,dtype=torch.float32)\n",
    "        spec_noisy = torch.tensor(cspec,dtype=torch.complex64)\n",
    "        mask = torch.tensor(cmask,dtype=torch.float32)\n",
    "        img_clean = torch.tensor(img_clean,dtype=torch.float32)\n",
    "        spec_clean = torch.tensor(spec_clean,dtype=torch.complex64)\n",
    "        return img_clean.unsqueeze(0), spec_clean.unsqueeze(0), img_noisy.unsqueeze(0), spec_noisy.unsqueeze(0), mask.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc8bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= MRIDenoisingDataset(img,spec)\n",
    "test_dataset = MRIDenoisingDataset(test_img,test_spec)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee5f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "\n",
    "        #Encoder\n",
    "        self.conv0 = nn.Conv2d(1, 48, 3, stride=1, padding=1)\n",
    "        self.conv1 = nn.Conv2d(48, 48, 3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(48, 48, 3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(48, 48, 3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(48, 48, 3, stride=1, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(48, 48, 3, stride=1, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(48, 48, 3, stride=1, padding=1)\n",
    "\n",
    "        #Decoder\n",
    "        self.upsample5 = nn.Upsample(scale_factor=2)\n",
    "        self.deconv5a = nn.Conv2d(96, 96, 3, stride=1, padding=1)\n",
    "        self.deconv5b = nn.Conv2d(96, 96, 3, stride=1, padding=1)\n",
    "\n",
    "        self.upsample4 = nn.Upsample(scale_factor=2)\n",
    "        self.deconv4a = nn.Conv2d(144, 96, 3, stride=1, padding=1)\n",
    "        self.deconv4b = nn.Conv2d(96, 96, 3, stride=1, padding=1)\n",
    "\n",
    "        self.upsample3 = nn.Upsample(scale_factor=2)\n",
    "        self.deconv3a = nn.Conv2d(144, 96, 3, stride=1, padding=1)\n",
    "        self.deconv3b = nn.Conv2d(96, 96, 3, stride=1, padding=1)\n",
    "\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2)\n",
    "        self.deconv2a = nn.Conv2d(144, 96, 3, stride=1, padding=1)\n",
    "        self.deconv2b = nn.Conv2d(96, 96, 3, stride=1, padding=1)\n",
    "\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2)\n",
    "        self.deconv1a = nn.Conv2d(97, 64, 3, stride=1, padding=1)\n",
    "        self.deconv1b = nn.Conv2d(64, 32, 3, stride=1, padding=1)\n",
    "        self.deconv1c = nn.Conv2d(32, 1, 3, stride=1, padding=1)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        original_in = F.pad(x, (0,1,0,1), mode='constant', value=-0.5)\n",
    "        x = F.leaky_relu(self.conv0(original_in))\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        pool1 = x\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        pool2 = x\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        pool3 = x\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "        pool4 = x\n",
    "        x = F.leaky_relu(self.conv5(x))\n",
    "        x = self.pool5(x)\n",
    "        x = F.leaky_relu(self.conv6(x))\n",
    "        x = self.upsample5(x)\n",
    "        x = torch.cat((x, pool4), 1)\n",
    "        x = F.leaky_relu(self.deconv5a(x))\n",
    "        x = F.leaky_relu(self.deconv5b(x))\n",
    "        x = self.upsample4(x)\n",
    "        x = torch.cat((x, pool3), 1)\n",
    "        x = F.leaky_relu(self.deconv4a(x))\n",
    "        x = F.leaky_relu(self.deconv4b(x))\n",
    "        x = self.upsample3(x)\n",
    "        x = torch.cat((x, pool2), 1)\n",
    "        x = F.leaky_relu(self.deconv3a(x))\n",
    "        x = F.leaky_relu(self.deconv3b(x))\n",
    "        x = self.upsample2(x)\n",
    "        x = torch.cat((x, pool1), 1)\n",
    "        x = F.leaky_relu(self.deconv2a(x))\n",
    "        x = F.leaky_relu(self.deconv2b(x))\n",
    "        x = self.upsample1(x)\n",
    "        x = torch.cat((x, original_in), 1)\n",
    "        x = F.leaky_relu(self.deconv1a(x))\n",
    "        x = F.leaky_relu(self.deconv1b(x))\n",
    "        x = self.deconv1c(x)\n",
    "        return x[:,:,:-1,:-1]\n",
    "\n",
    "model = Unet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (1, 255, 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0360462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "# the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=25, eta_min=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fftshift3d(x, ifft):\n",
    "    assert len(x.shape) == 3\n",
    "    s0 = (x.shape[1] // 2) + (0 if ifft else 1)\n",
    "    s1 = (x.shape[2] // 2) + (0 if ifft else 1)\n",
    "    x = torch.cat([x[:, s0:, :], x[:, :s0, :]], dim=1)\n",
    "    x = torch.cat([x[:, :, s1:], x[:, :, :s1]], dim=2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainLoader,testLoader, NUM_EPOCHS):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        with tqdm(trainLoader, unit=\"batch\") as tepoch:\n",
    "            for data in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "                img_clean,spec_clean, img_noisy, spec_noisy, mask = data\n",
    "                img_noisy = img_noisy.to(device)\n",
    "                img_clean = img_clean.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(img_noisy)\n",
    "                loss = loss_fn(outputs, img_clean)\n",
    "                # backpropagation\n",
    "                loss.backward()\n",
    "                # update the parameters\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss)    \n",
    "            \n",
    "            loss = running_loss / len(trainLoader)\n",
    "            train_loss.append(loss)\n",
    "        scheduler.step()\n",
    "        net.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            with tqdm(testLoader, unit=\"batch\") as tepoch:\n",
    "                for data in tepoch:\n",
    "                    img_clean,spec_clean, img_noisy, spec_noisy, mask = data\n",
    "                    img_noisy = img_noisy.to(device)\n",
    "                    img_clean = img_clean.to(device)\n",
    "                    outputs = net(img_noisy)\n",
    "                    outputs = torch.clamp(outputs, -.5, .5)\n",
    "                    loss = loss_fn(outputs, img_clean)\n",
    "                    val_loss += loss.item()\n",
    "                    tepoch.set_postfix(loss=loss)    \n",
    "                \n",
    "                loss = val_loss / len(testLoader)\n",
    "                test_loss.append(loss)\n",
    "        \n",
    "    \n",
    "    return train_loss,test_loss                          \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587890e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss,test_loss = train(model, dataloader, dataloader_test, 25)\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label='train loss')\n",
    "plt.plot(test_loss, label='test loss')\n",
    "plt.legend()\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17089da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "10*np.log10(1/0.0008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cdd78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[50],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_noisy, spec_noisy, mask = corrupt_data(img[50], spec[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c689b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_noisy=np.clip(img_noisy, -.5, .5)\n",
    "\n",
    "img_noisy = torch.tensor(img_noisy).to(device)\n",
    "img_noisy = img_noisy.unsqueeze(0).unsqueeze(0)\n",
    "reconstructed = model(img_noisy.to(device))\n",
    "reconstructed=torch.clamp(reconstructed, -0.5, 0.5)\n",
    "plt.imshow(np.squeeze(reconstructed.cpu().detach().numpy()),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e29289",
   "metadata": {},
   "source": [
    "## Rician Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e265023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rician_noise(img,noise_percent):\n",
    "    sigma =(noise_percent/100)*img.max().item()\n",
    "    noise1 = np.random.normal(0,sigma,img.shape)\n",
    "    noise2 = np.random.normal(0,sigma,img.shape)\n",
    "    noisy_img = np.sqrt((img+noise1)**2+noise2**2)\n",
    "    return noisy_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RicianMRIDenoisingDataset(Dataset):\n",
    "    def __init__(self,clean_images,t=64,corrupt_fn=rician_noise,augment_fn=None):\n",
    "        super(RicianMRIDenoisingDataset, self).__init__()\n",
    "        self.clean_images = clean_images\n",
    "        self.t = t\n",
    "        self.corrupt_fn = corrupt_fn\n",
    "        self.augment_fn = augment_fn\n",
    "    def __len__(self):\n",
    "        return len(self.clean_images)\n",
    "    def __getitem__(self, idx):\n",
    "        img_clean= self.clean_images[idx]+0.5 #now img in[0,1] range\n",
    "        # Data augmentation\n",
    "        if self.augment_fn:\n",
    "            img_clean,spec_clean = self.augment_fn(img_clean,spec_clean,t=self.t)        \n",
    "        # Corrupt data\n",
    "        cimg = self.corrupt_fn(img_clean,11)\n",
    "        img_noisy = torch.tensor(cimg,dtype=torch.float32).clamp(0,1)\n",
    "        img_clean = torch.tensor(img_clean,dtype=torch.float32).clamp(0,1)\n",
    "        return img_clean.unsqueeze(0), img_noisy.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b698ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Riciandataset= RicianMRIDenoisingDataset(img,spec)\n",
    "Riciantest_dataset = RicianMRIDenoisingDataset(test_img,test_spec)\n",
    "Riciandataloader = DataLoader(Riciandataset, batch_size=32, shuffle=True)\n",
    "Riciandataloader_test = DataLoader(Riciantest_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b01e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99dc599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RicianUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RicianUnet, self).__init__()\n",
    "\n",
    "        #Encoder\n",
    "        self.conv0 = nn.Conv2d(1, 48, 3, stride=1, padding=1)\n",
    "        self.conv1 = nn.Conv2d(48, 48, 3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(48, 48, 3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(48, 48, 3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(48, 48, 3, stride=1, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(48, 48, 3, stride=1, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(48, 48, 3, stride=1, padding=1)\n",
    "\n",
    "        #Decoder\n",
    "        self.upsample5 = nn.Upsample(scale_factor=2)\n",
    "        self.deconv5a = nn.Conv2d(96, 96, 3, stride=1, padding=1)\n",
    "        self.deconv5b = nn.Conv2d(96, 96, 3, stride=1, padding=1)\n",
    "\n",
    "        self.upsample4 = nn.Upsample(scale_factor=2)\n",
    "        self.deconv4a = nn.Conv2d(144, 96, 3, stride=1, padding=1)\n",
    "        self.deconv4b = nn.Conv2d(96, 96, 3, stride=1, padding=1)\n",
    "\n",
    "        self.upsample3 = nn.Upsample(scale_factor=2)\n",
    "        self.deconv3a = nn.Conv2d(144, 96, 3, stride=1, padding=1)\n",
    "        self.deconv3b = nn.Conv2d(96, 96, 3, stride=1, padding=1)\n",
    "\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2)\n",
    "        self.deconv2a = nn.Conv2d(144, 96, 3, stride=1, padding=1)\n",
    "        self.deconv2b = nn.Conv2d(96, 96, 3, stride=1, padding=1)\n",
    "\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2)\n",
    "        self.deconv1a = nn.Conv2d(97, 64, 3, stride=1, padding=1)\n",
    "        self.deconv1b = nn.Conv2d(64, 32, 3, stride=1, padding=1)\n",
    "        self.deconv1c = nn.Conv2d(32, 1, 3, stride=1, padding=1)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        original_in = F.pad(x, (0,1,0,1), mode='constant', value=-0.5)\n",
    "        x = F.leaky_relu(self.conv0(original_in))\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        pool1 = x\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        pool2 = x\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        pool3 = x\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "        pool4 = x\n",
    "        x = F.leaky_relu(self.conv5(x))\n",
    "        x = self.pool5(x)\n",
    "        x = F.leaky_relu(self.conv6(x))\n",
    "        x = self.upsample5(x)\n",
    "        x = torch.cat((x, pool4), 1)\n",
    "        x = F.leaky_relu(self.deconv5a(x))\n",
    "        x = F.leaky_relu(self.deconv5b(x))\n",
    "        x = self.upsample4(x)\n",
    "        x = torch.cat((x, pool3), 1)\n",
    "        x = F.leaky_relu(self.deconv4a(x))\n",
    "        x = F.leaky_relu(self.deconv4b(x))\n",
    "        x = self.upsample3(x)\n",
    "        x = torch.cat((x, pool2), 1)\n",
    "        x = F.leaky_relu(self.deconv3a(x))\n",
    "        x = F.leaky_relu(self.deconv3b(x))\n",
    "        x = self.upsample2(x)\n",
    "        x = torch.cat((x, pool1), 1)\n",
    "        x = F.leaky_relu(self.deconv2a(x))\n",
    "        x = F.leaky_relu(self.deconv2b(x))\n",
    "        x = self.upsample1(x)\n",
    "        x = torch.cat((x, original_in), 1)\n",
    "        x = F.leaky_relu(self.deconv1a(x))\n",
    "        x = F.leaky_relu(self.deconv1b(x))\n",
    "        x = self.deconv1c(x)\n",
    "        return x[:,:,:-1,:-1]\n",
    "\n",
    "rician_model = RicianUnet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss function\n",
    "rician_loss_fn = nn.MSELoss()\n",
    "# the optimizer\n",
    "rician_optimizer = optim.Adam(rician_model.parameters(), lr=0.001)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=25, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ricitrain(net, trainLoader,testLoader, NUM_EPOCHS):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        with tqdm(trainLoader, unit=\"batch\") as tepoch:\n",
    "            for data in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "                img_clean, img_noisy= data\n",
    "                img_noisy = img_noisy.to(device)\n",
    "                img_clean = img_clean.to(device)\n",
    "                rician_optimizer.zero_grad()\n",
    "                outputs = net(img_noisy)\n",
    "                loss = rician_loss_fn(outputs, img_clean)\n",
    "                # backpropagation\n",
    "                loss.backward()\n",
    "                # update the parameters\n",
    "                rician_optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss)    \n",
    "            \n",
    "            loss = running_loss / len(trainLoader)\n",
    "            train_loss.append(loss)\n",
    "        net.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            with tqdm(testLoader, unit=\"batch\") as tepoch:\n",
    "                for data in tepoch:\n",
    "                    img_clean,img_noisy = data\n",
    "                    img_noisy = img_noisy.to(device)\n",
    "                    img_clean = img_clean.to(device)\n",
    "                    outputs = net(img_noisy)\n",
    "                    outputs = torch.clamp(outputs, -0.5, 0.5)\n",
    "                    loss = rician_loss_fn(outputs, img_clean)\n",
    "                    val_loss += loss.item()\n",
    "                    tepoch.set_postfix(loss=loss)    \n",
    "                \n",
    "                loss = val_loss / len(testLoader)\n",
    "                test_loss.append(loss)\n",
    "    \n",
    "    return train_loss,test_loss                          \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a75875",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss,test_loss = ricitrain(rician_model, Riciandataloader, Riciandataloader_test, 25)\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label='train loss')\n",
    "plt.plot(test_loss, label='test loss')\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a06f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_loss, label='train loss')\n",
    "plt.plot(test_loss, label='test loss')\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
